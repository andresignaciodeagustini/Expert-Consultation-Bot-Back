# language.py - M√≥dulo mejorado de detecci√≥n y consistencia de idioma

import re
from typing import Dict, List, Optional, Tuple
import os

# Estado del m√≥dulo de idioma
class LanguageState:
    def __init__(self):
        self.current_language = 'en-US'  # Idioma actual
        self.language_history = []       # Historial de idiomas detectados
        self.conversation_context = []   # Historial simplificado de la conversaci√≥n
        self.max_history = 10           # N√∫mero m√°ximo de entradas en el historial
        
        # Mapeo de c√≥digos ISO a c√≥digos regionales
        self.language_map = {
            'es': 'es-ES', 'en': 'en-US', 'fr': 'fr-FR', 'de': 'de-DE', 
            'it': 'it-IT', 'pt': 'pt-PT', 'ru': 'ru-RU', 'zh': 'zh-CN', 
            'ja': 'ja-JP', 'ko': 'ko-KR', 'ar': 'ar-SA', 'hi': 'hi-IN',
            # M√°s idiomas comunes
            'nl': 'nl-NL', 'pl': 'pl-PL', 'tr': 'tr-TR', 'sv': 'sv-SE',
            'da': 'da-DK', 'fi': 'fi-FI', 'no': 'no-NO', 'cs': 'cs-CZ',
            'hu': 'hu-HU', 'el': 'el-GR', 'he': 'he-IL', 'th': 'th-TH',
            'vi': 'vi-VN', 'id': 'id-ID', 'ms': 'ms-MY', 'uk': 'uk-UA'
        }
        
        # Patrones ling√º√≠sticos espec√≠ficos (caracteres y patrones por idioma)
        self.language_patterns = {
            'es': {
                'chars': set('√°√©√≠√≥√∫√º√±¬ø¬°'),
                'common_words': ['el', 'la', 'los', 'las', 'un', 'una', 'y', 'o', 'pero', 'porque', 'como', 'qu√©', 'cu√°ndo', 'd√≥nde', 'qui√©n']
            },
            'en': {
                'chars': set(),  # Ingl√©s usa principalmente ASCII b√°sico
                'common_words': ['the', 'a', 'an', 'and', 'or', 'but', 'because', 'what', 'when', 'where', 'who', 'how']
            },
            'fr': {
                'chars': set('√©√®√™√´√†√¢√§√¥√∂√π√ª√º√ø√ß√â√à√ä√ã√Ä√Ç√Ñ√î√ñ√ô√õ√ú≈∏√á'),
                'common_words': ['le', 'la', 'les', 'un', 'une', 'et', 'ou', 'mais', 'parce', 'que', 'quand', 'o√π', 'qui', 'comment']
            },
            'de': {
                'chars': set('√§√∂√º√ü√Ñ√ñ√ú'),
                'common_words': ['der', 'die', 'das', 'ein', 'eine', 'und', 'oder', 'aber', 'weil', 'was', 'wann', 'wo', 'wer', 'wie']
            },
            'it': {
                'chars': set('√†√®√©√¨√≤√π'),
                'common_words': ['il', 'lo', 'la', 'i', 'gli', 'le', 'un', 'uno', 'una', 'e', 'o', 'ma', 'perch√©', 'cosa', 'quando', 'dove', 'chi', 'come']
            },
            'pt': {
                'chars': set('√°√†√¢√£√©√™√≠√≥√¥√µ√∫√º√ß'),
                'common_words': ['o', 'a', 'os', 'as', 'um', 'uma', 'e', 'ou', 'mas', 'porque', 'que', 'quando', 'onde', 'quem', 'como']
            },
            'ru': {
                'chars': set('–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è–ê–ë–í–ì–î–ï–Å–ñ–ó–ò–ô–ö–õ–ú–ù–û–ü–†–°–¢–£–§–•–¶–ß–®–©–™–´–¨–≠–Æ–Ø'),
                'common_words': ['–∏', '–≤', '–Ω–µ', '–Ω–∞', '—è', '—á—Ç–æ', '—Ç–æ—Ç', '–±—ã—Ç—å', '—Å', '–∞', '–≤–µ—Å—å', '—ç—Ç–æ', '–∫–∞–∫', '–æ–Ω–∞', '–ø–æ', '–Ω–æ', '–æ–Ω–∏', '–∫', '—É', '—Ç—ã']
            },
            'zh': {
                'chars': set(),  # Caracteres chinos (Unicode del bloque CJK)
                'common_words': ['ÁöÑ', '‰∏Ä', 'ÊòØ', 'Âú®', '‰∏ç', '‰∫Ü', 'Êúâ', 'Âíå', '‰∫∫', 'Ëøô', '‰∏≠', 'Â§ß', '‰∏∫', '‰∏ä', '‰∏™', 'ÂõΩ', 'Êàë', '‰ª•', 'Ë¶Å', '‰ªñ']
            },
            'ja': {
                'chars': set('„ÅÇ„ÅÑ„ÅÜ„Åà„Åä„Åã„Åç„Åè„Åë„Åì„Åï„Åó„Åô„Åõ„Åù„Åü„Å°„Å§„Å¶„Å®„Å™„Å´„Å¨„Å≠„ÅÆ„ÅØ„Å≤„Åµ„Å∏„Åª„Åæ„Åø„ÇÄ„ÇÅ„ÇÇ„ÇÑ„ÇÜ„Çà„Çâ„Çä„Çã„Çå„Çç„Çè„Çí„Çì„Åå„Åé„Åê„Åí„Åî„Åñ„Åò„Åö„Åú„Åû„Å†„Å¢„Å•„Åß„Å©„Å∞„Å≥„Å∂„Åπ„Åº„Å±„Å¥„Å∑„Å∫„ÅΩ„Ç¢„Ç§„Ç¶„Ç®„Ç™„Ç´„Ç≠„ÇØ„Ç±„Ç≥„Çµ„Ç∑„Çπ„Çª„ÇΩ„Çø„ÉÅ„ÉÑ„ÉÜ„Éà„Éä„Éã„Éå„Éç„Éé„Éè„Éí„Éï„Éò„Éõ„Éû„Éü„É†„É°„É¢„É§„É¶„É®„É©„É™„É´„É¨„É≠„ÉØ„É≤„É≥„Ç¨„ÇÆ„Ç∞„Ç≤„Ç¥„Ç∂„Ç∏„Ç∫„Çº„Çæ„ÉÄ„ÉÇ„ÉÖ„Éá„Éâ„Éê„Éì„Éñ„Éô„Éú„Éë„Éî„Éó„Éö„Éù'),
                'common_words': ['„ÅØ', '„ÅÆ', '„Å´', '„Çí', '„Åü', '„Åå', '„Åß', '„Å¶', '„Å®', '„Åó', '„Çå', '„Åï', '„ÅÇ„Çã', '„ÅÑ„Çã', '„ÇÇ', '„Åô„Çã', '„Åã„Çâ', '„Å™', '„Åì„Å®', '„Å®„Åó„Å¶']
            },
            'ko': {
                'chars': set('„Ñ±„Ñ≤„Ñ¥„Ñ∑„Ñ∏„Ñπ„ÖÅ„ÖÇ„ÖÉ„ÖÖ„ÖÜ„Öá„Öà„Öâ„Öä„Öã„Öå„Öç„Öé„Öè„Öê„Öë„Öí„Öì„Öî„Öï„Öñ„Öó„Öò„Öô„Öö„Öõ„Öú„Öù„Öû„Öü„Ö†„Ö°„Ö¢„Ö£'),
                'common_words': ['Ïù¥', 'Í∑∏', 'Ï†Ä', 'Í≤É', 'Ïàò', 'Î•º', 'Ïóê', 'Ïùò', 'Ìïú', 'Ïûê', 'ÏóêÏÑú', 'Í≥º', 'Îäî', 'ÏúºÎ°ú', 'ÌïòÎã§', 'Í∞Ä', 'ÏùÑ', 'Î°ú', 'Ïù∏', 'ÎìØ']
            },
            'ar': {
                'chars': set('ÿ°ÿ¢ÿ£ÿ§ÿ•ÿ¶ÿßÿ®ÿ©ÿ™ÿ´ÿ¨ÿ≠ÿÆÿØÿ∞ÿ±ÿ≤ÿ≥ÿ¥ÿµÿ∂ÿ∑ÿ∏ÿπÿ∫ŸÅŸÇŸÉŸÑŸÖŸÜŸáŸàŸâŸäŸãŸåŸçŸéŸèŸêŸëŸí'),
                'common_words': ['ŸÅŸä', 'ŸÖŸÜ', 'ÿπŸÑŸâ', 'ÿ•ŸÑŸâ', 'ÿπŸÜ', 'ŸÖÿπ', 'Ÿáÿ∞ÿß', 'ÿ£ŸÜ', 'ŸÑÿß', 'ŸÖÿß', 'ŸáŸà', 'Ÿà', 'ÿ£Ÿà', 'ÿ´ŸÖ', 'ŸÇÿØ', 'ŸÉÿßŸÜ', 'ÿ•ÿ∞ÿß', 'ŸáŸÑ', 'ŸÑŸÖ', 'ŸÑŸÜ']
            },
            'hi': {
                'chars': set('‡§Ö‡§Ü‡§á‡§à‡§â‡§ä‡§ã‡§è‡§ê‡§ì‡§î‡§ï‡§ñ‡§ó‡§ò‡§ô‡§ö‡§õ‡§ú‡§ù‡§û‡§ü‡§†‡§°‡§¢‡§£‡§§‡§•‡§¶‡§ß‡§®‡§™‡§´‡§¨‡§≠‡§Æ‡§Ø‡§∞‡§≤‡§µ‡§∂‡§∑‡§∏‡§π‡§Ω‡§æ‡§ø‡•Ä‡•Å‡•Ç‡•É‡•á‡•à‡•ã‡•å‡•ç‡§Ç‡§É‡§Å'),
                'common_words': ['‡§ï‡§æ', '‡§ï‡•á', '‡§Æ‡•á‡§Ç', '‡§π‡•à', '‡§ï‡•Ä', '‡§î‡§∞', '‡§ï‡•ã', '‡§∏‡•á', '‡§™‡§∞', '‡§è‡§ï', '‡§Ø‡§π', '‡§π‡•à‡§Ç', '‡§•‡§æ', '‡§µ‡§π', '‡§®‡•á', '‡§π‡•ã', '‡§ú‡•ã', '‡§ï‡§ø', '‡§•‡•á', '‡§Ø‡§æ']
            },
            'nl': {
                'chars': set('√°√†√§√©√®√´√™√≠√¨√Ø√Æ√≥√≤√∂√¥√∫√π√º√ª√Å√Ä√Ñ√â√à√ã√ä√ç√å√è√é√ì√í√ñ√î√ö√ô√ú√õ'),
                'common_words': ['de', 'het', 'een', 'in', 'is', 'dat', 'op', 'te', 'en', 'van', 'voor', 'met', 'zijn', 'er', 'niet', 'aan', 'om', 'ook', 'als', 'bij']
            },
            'pl': {
                'chars': set('ƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈ºƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª'),
                'common_words': ['i', 'w', 'nie', 'na', 'siƒô', 'z', 'do', 'to', '≈ºe', 'a', 'o', 'jak', 'ale', 'po', 'co', 'tak', 'za', 'od', 'przez', 'ten']
            },
            'tr': {
                'chars': set('√ßƒüƒ±√∂≈ü√º√áƒûƒ∞√ñ≈û√ú'),
                'common_words': ['ve', 'bir', 'bu', 'i√ßin', 'ile', 'de', 'da', 'o', 'ki', 'ne', 'ben', 'sen', 'biz', 'siz', 'ama', '√ß√ºnk√º', 'eƒüer', 'nasƒ±l', 'gibi', 'kadar']
            },
            'sv': {
                'chars': set('√•√§√∂√Ö√Ñ√ñ'),
                'common_words': ['och', 'i', 'att', 'en', 'ett', 'som', 'p√•', '√§r', 'av', 'f√∂r', 'med', 'den', 'till', 'det', 'inte', 'om', 'har', 'de', 'jag', 'du']
            },
            'da': {
                'chars': set('√¶√∏√•√Ü√ò√Ö'),
                'common_words': ['og', 'i', 'at', 'en', 'et', 'som', 'p√•', 'er', 'af', 'for', 'med', 'den', 'til', 'det', 'ikke', 'om', 'har', 'de', 'jeg', 'du']
            },
            'fi': {
                'chars': set('√§√∂√Ñ√ñ'),
                'common_words': ['ja', 'on', 'ei', 'se', 'ett√§', 'h√§n', 'oli', 'min√§', 'sin√§', 'me', 'te', 'he', 'mit√§', 'mutta', 'kun', 'jos', 'niin', 'kuin', 'vain', 'my√∂s']
            },
            'no': {
                'chars': set('√¶√∏√•√Ü√ò√Ö'),
                'common_words': ['og', 'i', '√•', 'en', 'et', 'som', 'p√•', 'er', 'av', 'for', 'med', 'den', 'til', 'det', 'ikke', 'om', 'har', 'de', 'jeg', 'du']
            },
            'cs': {
                'chars': set('√°ƒçƒè√©ƒõ√≠≈à√≥≈ô≈°≈•√∫≈Ø√Ω≈æ√Åƒåƒé√âƒö√ç≈á√ì≈ò≈†≈§√ö≈Æ√ù≈Ω'),
                'common_words': ['a', 'v', 'je', 'se', 'na', '≈æe', 'to', 's', 'z', 'do', 'o', 'b√Ωt', 'ten', 'i', 'k', 'm√≠t', 'kter√Ω', 'jak', 'ale', 'za']
            },
            'hu': {
                'chars': set('√°√©√≠√≥√∂≈ë√∫√º≈±√Å√â√ç√ì√ñ≈ê√ö√ú≈∞'),
                'common_words': ['a', 'az', '√©s', 'van', 'hogy', 'nem', 'egy', 'ez', 'de', 'is', '√©n', 'te', '≈ë', 'mi', 'ti', '≈ëk', 'meg', 'csak', 'm√°r', 'm√©g']
            },
            'el': {
                'chars': set('Œ±Œ≤Œ≥Œ¥ŒµŒ∂Œ∑Œ∏ŒπŒ∫ŒªŒºŒΩŒæŒøœÄœÅœÉœÇœÑœÖœÜœáœàœâŒ¨Œ≠ŒÆŒØœåœçœéœäœãŒêŒ∞ŒëŒíŒìŒîŒïŒñŒóŒòŒôŒöŒõŒúŒùŒûŒüŒ†Œ°Œ£Œ§Œ•Œ¶ŒßŒ®Œ©ŒÜŒàŒâŒäŒåŒéŒèŒ™Œ´'),
                'common_words': ['Œ∫Œ±Œπ', 'œÑŒø', 'œÑŒ∑œÇ', 'Œø', 'Œ∑', 'œÉŒµ', 'œÄŒøœÖ', 'Œ±œÄœå', 'ŒºŒµ', 'Œ≥ŒπŒ±', 'Œ≠ŒΩŒ±', 'ŒµŒØŒΩŒ±Œπ', 'œÑŒ±', 'Œ¥ŒµŒΩ', 'œÉœÑŒø', 'ŒΩŒ±', 'œÑŒøŒΩ', 'œÑŒ∑ŒΩ', 'œÉœÑŒ∑ŒΩ', 'œÑœâŒΩ']
            },
            'he': {
                'chars': set('◊ê◊ë◊í◊ì◊î◊ï◊ñ◊ó◊ò◊ô◊ö◊õ◊ú◊ù◊û◊ü◊†◊°◊¢◊£◊§◊•◊¶◊ß◊®◊©◊™'),
                'common_words': ['◊ê◊™', '◊©◊ú', '◊î◊ï◊ê', '◊ñ◊î', '◊¢◊ù', '◊¢◊ú', '◊ê◊†◊ô', '◊ú◊ê', '◊õ◊ô', '◊î◊ô◊ê', '◊í◊ù', '◊ê◊ï', '◊ê◊ë◊ú', '◊û◊î', '◊ê◊ù', '◊®◊ß', '◊õ◊ú', '◊ô◊©', '◊õ◊û◊ï', '◊ê◊ñ']
            },
            'th': {
                'chars': set('‡∏Å‡∏Ç‡∏É‡∏Ñ‡∏Ö‡∏Ü‡∏á‡∏à‡∏â‡∏ä‡∏ã‡∏å‡∏ç‡∏é‡∏è‡∏ê‡∏ë‡∏í‡∏ì‡∏î‡∏ï‡∏ñ‡∏ó‡∏ò‡∏ô‡∏ö‡∏õ‡∏ú‡∏ù‡∏û‡∏ü‡∏†‡∏°‡∏¢‡∏£‡∏§‡∏•‡∏¶‡∏ß‡∏®‡∏©‡∏™‡∏´‡∏¨‡∏≠‡∏Æ‡∏Ø‡∏∞‡∏±‡∏≤‡∏≥‡∏¥‡∏µ‡∏∂‡∏∑‡∏∏‡∏π‡πÄ‡πÅ‡πÇ‡πÉ‡πÑ‡πÖ‡πÜ‡πá‡πà‡πâ‡πä‡πã‡πå‡πç‡πé‡πè‡πê‡πë‡πí‡πì‡πî‡πï‡πñ‡πó‡πò‡πô‡πö‡πõ'),
                'common_words': ['‡πÅ‡∏•‡∏∞', '‡∏Ç‡∏≠‡∏á', '‡πÉ‡∏ô', '‡∏ó‡∏µ‡πà', '‡∏°‡∏µ', '‡∏Å‡∏≤‡∏£', '‡πÑ‡∏°‡πà', '‡πÄ‡∏õ‡πá‡∏ô', '‡πÉ‡∏´‡πâ', '‡∏ß‡πà‡∏≤', '‡πÑ‡∏î‡πâ', '‡∏à‡∏∞', '‡∏°‡∏≤', '‡∏Å‡∏±‡∏ö', '‡∏Å‡πá', '‡πÅ‡∏ï‡πà', '‡∏Ñ‡∏∑‡∏≠', '‡∏≠‡∏¢‡∏π‡πà', '‡∏ô‡∏µ‡πâ', '‡πÄ‡∏û‡∏∑‡πà‡∏≠']
            },
            'vi': {
                'chars': set('√†√°√¢√£√®√©√™√¨√≠√≤√≥√¥√µ√π√∫√ΩƒÉƒëƒ©≈©∆°∆∞·∫°·∫£·∫•·∫ß·∫©·∫´·∫≠·∫Ø·∫±·∫≥·∫µ·∫∑·∫π·∫ª·∫Ω·∫ø·ªÅ·ªÉ·ªÖ·ªá·ªâ·ªã·ªç·ªè·ªë·ªì·ªï·ªó·ªô·ªõ·ªù·ªü·ª°·ª£·ª•·ªß·ª©·ª´·ª≠·ªØ·ª±·ª≥·ªµ·ª∑·ªπ'),
                'common_words': ['v√†', 'c·ªßa', 'c√≥', 'trong', 'l√†', 'ƒë∆∞·ª£c', 'ƒë·ªÉ', 'kh√¥ng', 'm·ªôt', 'cho', 'v·ªõi', 'ng∆∞·ªùi', 't·ª´', 'ƒë√£', 'v·ªÅ', 'ƒë√≥', 'c√°c', 't√¥i', 'nh∆∞', 'khi']
            },
            'id': {
                'chars': set(),  # Principalmente usa caracteres ASCII con algunas excepciones
                'common_words': ['dan', 'yang', 'di', 'dengan', 'itu', 'untuk', 'pada', 'dari', 'tidak', 'ini', 'dalam', 'adalah', 'ke', 'ada', 'akan', 'oleh', 'saya', 'kamu', 'mereka', 'bisa']
            },
            'ms': {
                'chars': set(),  # Principalmente usa caracteres ASCII con algunas excepciones
                'common_words': ['dan', 'yang', 'di', 'dengan', 'itu', 'untuk', 'pada', 'dari', 'tidak', 'ini', 'dalam', 'adalah', 'ke', 'ada', 'akan', 'oleh', 'saya', 'kamu', 'mereka', 'boleh']
            },
            'uk': {
                'chars': set('–∞–±–≤–≥“ë–¥–µ—î–∂–∑–∏—ñ—ó–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—å—é—è–ê–ë–í–ì“ê–î–ï–Ñ–ñ–ó–ò–Ü–á–ô–ö–õ–ú–ù–û–ü–†–°–¢–£–§–•–¶–ß–®–©–¨–Æ–Ø'),
                'common_words': ['—ñ', '–≤', '–Ω–µ', '–Ω–∞', '—è', '—â–æ', '—Ç–æ–π', '–±—É—Ç–∏', '–∑', '–∞', '–≤–µ—Å—å', '—Ü–µ', '—è–∫', '–≤–æ–Ω–∞', '–ø–æ', '–∞–ª–µ', '–≤–æ–Ω–∏', '–¥–æ', '—É', '—Ç–∏']
            }
        }
        
        # Palabras ambiguas que son similares en varios idiomas
        self.ambiguous_words = set([
            # Afirmaci√≥n/negaci√≥n
            'no', 'yes', 'si', 'oui', 'non', 'ja', 'nein', 'ok', 'okay',
            
            # Saludos cortos
            'hi', 'hey', 'bye', 'hola', 'adios', 'ciao', 'salut', 'hallo', '–ø—Ä–∏–≤–µ—Ç', '„Åì„Çì„Å´„Å°„ÅØ', '‰Ω†Â•Ω', '‡§®‡§Æ‡§∏‡•ç‡§§‡•á',
            
            # N√∫meros y medidas
            'one', 'two', 'three', 'uno', 'dos', 'tres', 'un', 'deux', 'trois', 'eins', 'zwei', 'drei',
            
            # Pronombres
            'i', 'you', 'he', 'she', 'we', 'they', 'yo', 'tu', '√©l', 'ella', 'je', 'tu', 'il', 'elle',
            'ich', 'du', 'er', 'sie', 'wir', 'ihr', 'sie', 'io', 'tu', 'lui', 'lei', 'noi', 'voi', 'loro',
            
            # Palabras t√©cnicas/internacionales
            'tech', 'it', 'software', 'data', 'cloud', 'web', 'online', 'app', 'net', 'digital',
            'email', 'internet', 'wifi', 'blog', 'post', 'chat', 'video', 'audio', 'photo', 'code',
            'computer', 'laptop', 'server', 'database', 'api', 'file', 'system', 'network', 'tech',
            'smart', 'phone', 'mobile', 'tablet', 'desktop', 'robot', 'cyber', 'crypto', 'blockchain',
            
            # Marcas y productos internacionales
            'google', 'facebook', 'twitter', 'instagram', 'youtube', 'tiktok', 'whatsapp', 'netflix',
            'amazon', 'apple', 'microsoft', 'samsung', 'huawei', 'xiaomi', 'spotify', 'uber', 'gmail',
            
            # T√©rminos t√©cnicos comunes
            'html', 'css', 'javascript', 'python', 'java', 'php', 'sql', 'c++', 'ruby', 'swift',
            'react', 'angular', 'vue', 'node', 'linux', 'windows', 'ios', 'android', 'excel', 'word',
            
            # Expresiones breves comunes
            'ok', 'wow', 'cool', 'super', 'nice', 'good', 'bad', 'lol', 'haha', 'omg',
            
            # Emojis y s√≠mbolos (como strings)
            ':)', ':(', ':D', ';)', '?', '!', '...', '‚ô•', 'üëç', 'üôè', 'üòä', 'üòÇ', 'ü§î', 'üëÄ', 'üî•'
        ])

# Instancia global del estado de idioma
_language_state = LanguageState()

def normalize_language_code(language_code: str) -> str:
    """
    Normaliza el c√≥digo de idioma al formato est√°ndar (xx-XX)
    
    Args:
        language_code: C√≥digo de idioma en cualquier formato
        
    Returns:
        C√≥digo de idioma normalizado
    """
    if not language_code or not isinstance(language_code, str):
        return _language_state.current_language
    
    # Eliminar espacios y convertir a min√∫sculas
    language_code = language_code.strip().lower()
    
    # Si ya tiene formato regional (xx-XX)
    if '-' in language_code and len(language_code) >= 4:
        base_code = language_code.split('-')[0]
        # Verificar si el c√≥digo base existe en nuestro mapeo
        if base_code in _language_state.language_map:
            return _language_state.language_map[base_code]
        return language_code
    
    # Si es solo c√≥digo ISO (xx)
    if len(language_code) == 2:
        return _language_state.language_map.get(
            language_code, 
            f"{language_code}-{language_code.upper()}"
        )
    
    # Si no se puede normalizar, devolver el idioma actual
    return _language_state.current_language

def is_text_ambiguous(text: str) -> bool:
    """
    Determina si un texto es ambiguo para la detecci√≥n de idioma
    
    Args:
        text: Texto a analizar
        
    Returns:
        True si el texto es ambiguo, False en caso contrario
    """
    # Eliminar espacios y convertir a min√∫sculas
    cleaned_text = text.strip().lower()
    
    # Criterios de ambig√ºedad:
    # 1. Texto muy corto
    if len(cleaned_text) <= 4:
        return True
        
    # 2. Texto es una palabra ambigua conocida
    if cleaned_text in _language_state.ambiguous_words:
        return True
    
    # 3. Texto contiene principalmente s√≠mbolos o n√∫meros
    alpha_ratio = sum(c.isalpha() for c in cleaned_text) / max(len(cleaned_text), 1)
    if alpha_ratio < 0.5:
        return True
        
    # 4. Texto es muy corto (una sola palabra)
    if len(cleaned_text.split()) <= 1:
        return True
        
    return False

def analyze_language_patterns(text: str) -> Dict[str, float]:
    """
    Analiza los patrones ling√º√≠sticos en el texto para determinar el idioma probable
    
    Args:
        text: Texto a analizar
        
    Returns:
        Diccionario con puntuaciones para cada idioma
    """
    # Texto limpio para an√°lisis
    cleaned_text = text.lower()
    words = re.findall(r'\b\w+\b', cleaned_text)
    
    scores = {}
    
    # Analizar patrones para cada idioma
    for lang, patterns in _language_state.language_patterns.items():
        score = 0.0
        
        # Puntuaci√≥n por caracteres espec√≠ficos
        char_matches = sum(1 for c in cleaned_text if c in patterns['chars'])
        char_score = char_matches / max(len(cleaned_text), 1) * 100
        
        # Puntuaci√≥n por palabras comunes
        word_matches = sum(1 for word in words if word in patterns['common_words'])
        word_score = word_matches / max(len(words), 1) * 100
        
        # Combinaci√≥n de puntuaciones (caracteres tienen m√°s peso para idiomas distintivos)
        if patterns['chars']:
            score = (char_score * 0.7) + (word_score * 0.3)
        else:
            score = word_score
            
        scores[lang] = score
    
    return scores

def evaluate_context_consistency(detected_lang: str) -> float:
    """
    Eval√∫a la consistencia del idioma detectado con el contexto previo
    
    Args:
        detected_lang: C√≥digo ISO del idioma detectado
        
    Returns:
        Puntuaci√≥n de consistencia (0-1)
    """
    if not _language_state.language_history:
        return 0.5  # Neutral si no hay historial
    
    # Contar frecuencia de idiomas en el historial
    lang_counts = {}
    for lang in _language_state.language_history:
        base_lang = lang.split('-')[0]
        lang_counts[base_lang] = lang_counts.get(base_lang, 0) + 1
    
    # Calcular consistencia basada en frecuencia
    detected_base = detected_lang.split('-')[0]
    total_entries = len(_language_state.language_history)
    
    consistency = lang_counts.get(detected_base, 0) / total_entries
    
    # Dar m√°s peso a entradas recientes
    if _language_state.language_history and detected_base == _language_state.language_history[-1].split('-')[0]:
        consistency += 0.2  # Bonus por coincidencia con el √∫ltimo idioma
    
    return min(consistency, 1.0)  # Limitar a 1.0

def detect_language(text: str) -> Tuple[str, float]:
    """
    Detecta el idioma del texto usando an√°lisis de patrones ling√º√≠sticos
    
    Args:
        text: Texto a analizar
        
    Returns:
        Tuple con (c√≥digo_idioma, confianza)
    """
    # Si el texto est√° vac√≠o, mantener el idioma actual
    if not text or not text.strip():
        return _language_state.current_language, 1.0
    
    # Para textos ambiguos, preferir el idioma actual
    if is_text_ambiguous(text):
        return _language_state.current_language, 0.8
    
    # Analizar patrones ling√º√≠sticos
    lang_scores = analyze_language_patterns(text)
    
    if not lang_scores:
        return _language_state.current_language, 0.5
    
    # Encontrar el idioma con mayor puntuaci√≥n
    best_lang = max(lang_scores.items(), key=lambda x: x[1])
    lang_code, score = best_lang
    
    # Normalizar la confianza a un rango de 0-1
    confidence = min(score / 100, 1.0)
    
    # Evaluar consistencia con el contexto
    context_score = evaluate_context_consistency(lang_code)
    
    # Combinar puntuaci√≥n de patrones y consistencia
    combined_confidence = (confidence * 0.7) + (context_score * 0.3)
    
    # Si la confianza es muy baja, mantener el idioma actual
    if combined_confidence < 0.4:
        return _language_state.current_language, 0.5
    
    # Obtener c√≥digo regional del idioma
    detected_language = normalize_language_code(lang_code)
    
    return detected_language, combined_confidence

def update_last_detected_language(language: Optional[str] = None, text: Optional[str] = None) -> str:
    """
    Actualiza el √∫ltimo idioma detectado con l√≥gica avanzada
    
    Args:
        language: C√≥digo de idioma proporcionado externamente (opcional)
        text: Texto para detectar idioma (si no se proporciona language)
        
    Returns:
        C√≥digo de idioma actualizado
    """
    detected_language = _language_state.current_language
    confidence = 0.5
    
    # Si se proporciona texto pero no idioma, detectar idioma
    if text and not language:
        detected_language, confidence = detect_language(text)
    # Si se proporciona idioma, normalizar
    elif language:
        detected_language = normalize_language_code(language)
        confidence = 0.9  # Alta confianza para detecciones externas
    
    # Actualizar el historial solo si hay suficiente confianza
    if confidence >= 0.5:
        _language_state.current_language = detected_language
        _language_state.language_history.append(detected_language)
        
        # Mantener historial con tama√±o m√°ximo
        if len(_language_state.language_history) > _language_state.max_history:
            _language_state.language_history.pop(0)
    
    # Actualizar contexto de conversaci√≥n si se proporciona texto
    if text:
        _language_state.conversation_context.append(text[:100])  # Guardar solo los primeros 100 caracteres
        
        # Mantener contexto con tama√±o m√°ximo
        if len(_language_state.conversation_context) > _language_state.max_history:
            _language_state.conversation_context.pop(0)
    
    return _language_state.current_language

def get_last_detected_language() -> str:
    """
    Obtiene el √∫ltimo idioma detectado
    
    Returns:
        C√≥digo de idioma actual
    """
    return _language_state.current_language

def get_language_history() -> List[str]:
    """
    Obtiene el historial de idiomas detectados
    
    Returns:
        Lista de c√≥digos de idioma detectados
    """
    return _language_state.language_history.copy()

def reset_last_detected_language(default_language: str = 'en-US') -> str:
    """
    Resetear al idioma por defecto
    
    :param default_language: Idioma por defecto a establecer
    :return: Idioma por defecto
    """
    return reset_language_state(default_language)

def reset_language_state(default_language: str = 'en-US') -> str:
    """
    Reinicia el estado del idioma
    
    Args:
        default_language: Idioma por defecto
        
    Returns:
        Idioma por defecto establecido
    """
    _language_state.current_language = default_language
    _language_state.language_history = []
    _language_state.conversation_context = []
    return default_language

def process_message(text: str) -> Dict:
    """
    Procesa un mensaje completo con detecci√≥n de idioma avanzada
    
    Args:
        text: Texto del mensaje
        
    Returns:
        Diccionario con informaci√≥n de detecci√≥n
    """
    previous_language = _language_state.current_language
    
    # Si el texto est√° vac√≠o, mantener el idioma actual
    if not text or not text.strip():
        return {
            "success": True,
            "text": text,
            "detected_language": previous_language,
            "previous_language": previous_language,
            "confidence": 1.0
        }
    
    # Para textos ambiguos, preferir mantener el idioma actual
    if is_text_ambiguous(text):
        return {
            "success": True,
            "text": text,
            "detected_language": previous_language,
            "previous_language": previous_language,
            "confidence": 0.8,
            "is_ambiguous": True
        }
    
    # Detectar idioma con an√°lisis de patrones
    detected_language, confidence = detect_language(text)
    
    # Actualizar el estado global
    updated_language = update_last_detected_language(detected_language)
    
    return {
        "success": True,
        "text": text,
        "detected_language": updated_language,
        "previous_language": previous_language,
        "confidence": confidence,
        "is_ambiguous": False
    }